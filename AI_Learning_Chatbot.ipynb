{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWC0Cd0fsYbJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install \\\n",
        "    langchain_community\\\n",
        "    langchain_text_splitters\\\n",
        "    langchain_openai\\\n",
        "    langchain_pinecone\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Iox_YO-tXFA",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcoaC5uRsMuh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhSSLjlOtFCc"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlretrieve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3_iPpaDtHmg"
      },
      "outputs": [],
      "source": [
        "files = [\n",
        "    \"https://www.deeplearningbook.org/front_matter.pdf\",\n",
        "    \"https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\",\n",
        "    \"https://www.academia.dk/BiologiskAntropologi/Epidemiologi/DataMining/Artificial_Intelligence-A_Guide_to_Intelligent_Systems.pdf\",\n",
        "    \"https://mrce.in/ebooks/AI%20Foundations%20of%20Computational%20Agents%203rd%20Ed.pdf\"\n",
        "]\n",
        "os.makedirs(\"data\",exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK09hl64tKTI"
      },
      "outputs": [],
      "source": [
        "for url in files:\n",
        "    file_path=os.path.join(\"data\", url.rpartition(\"/\")[2])\n",
        "    urlretrieve(url,file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1XTagZ-tNRi"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFDirectoryLoader(\"./data/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0XbbX9_tRCB"
      },
      "outputs": [],
      "source": [
        "docs_before_split=loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3Ele0ygt6rZ"
      },
      "outputs": [],
      "source": [
        "docs_before_split[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr8mmGpW1F1c"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY']=userdata.get('OPENAI_API_KEY')\n",
        "os.environ['PINECONE_API_KEY']=userdata.get('PINECONE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMeMVVrx11fW"
      },
      "outputs": [],
      "source": [
        "embeddings =OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        ")\n",
        "\n",
        "index_name=\"chatbot\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03gBuJJz2Gub"
      },
      "outputs": [],
      "source": [
        "text_splitter=RecursiveCharacterTextSplitter()\n",
        "split_docs=text_splitter.split_documents(docs_before_split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpz9Y9lP235I"
      },
      "outputs": [],
      "source": [
        "split_docs[25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw0tWS-P26iV"
      },
      "outputs": [],
      "source": [
        "vectorestore=PineconeVectorStore.from_documents(split_docs,embeddings,index_name=index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7O40BCS3luh"
      },
      "outputs": [],
      "source": [
        "query = \"What is the neural network?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLrXssZv4VIY"
      },
      "outputs": [],
      "source": [
        "similar_docs=vectorestore.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Edz5Z-K64dBB"
      },
      "outputs": [],
      "source": [
        "similar_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVOEhbcL4vIO"
      },
      "outputs": [],
      "source": [
        "llm= ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0.3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EH4Jzl537NJl"
      },
      "outputs": [],
      "source": [
        "qa= RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorestore.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHsvtKObJeZG"
      },
      "outputs": [],
      "source": [
        "qa.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpM2oVPzI1-d"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph"
      ],
      "metadata": {
        "id": "lNZr5wdX-N-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "\n",
        "def chat_with_bot(user_message, history):\n",
        "    conversation_chain = RunnableWithMessageHistory(llm=llm, history=history)\n",
        "    if not user_message.strip():\n",
        "        return gr.update(value=\"Message cannot be empty!\"), history\n",
        "\n",
        "\n",
        "    response = conversation_chain.predict(input=user_message)\n",
        "\n",
        "    history.append((user_message, response))\n",
        "\n",
        "    return gr.update(value=\"\"), history\n",
        "\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"# AI Learning Chatbot\")\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"Chat Interface\")\n",
        "    user_input = gr.Textbox(label=\"Your Message\", placeholder=\"Type your question here...\", lines=1)\n",
        "    clear_button = gr.Button(\"Clear Chat\")\n",
        "    chat_history = []\n",
        "\n",
        "    def user(user_message, history):\n",
        "        if not user_message.strip():\n",
        "            return gr.update(value=\"Message cannot be empty!\"), history\n",
        "\n",
        "    user_input.submit(chat_with_bot, [user_input, chatbot], [user_input, chatbot], queue=True)\n",
        "    clear_button.click(lambda: [], None, chatbot, queue=False) # changed lambda to return []\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.launch(debug=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "7Mdx-2P1-S9S",
        "outputId": "e71fbaa0-61b6-4cf3-ee5c-1d137def6bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/components/chatbot.py:273: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://c4c6bf8b90fc4ca011.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c4c6bf8b90fc4ca011.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}